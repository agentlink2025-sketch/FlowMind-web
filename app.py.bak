from flask import Flask, request, jsonify, Response
from flask_cors import CORS
from zhipuai import ZhipuAI
import json
import requests
from requests.exceptions import Timeout
import time
import httpx
from openai import OpenAI
from langchain_core.prompts import ChatPromptTemplate
import os
import pandas as pd
import uuid
from werkzeug.utils import secure_filename
from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime
from pptx import Presentation
from pptx.util import Pt
import tempfile
from ai_ppt import AIPPT

app = Flask(__name__)
CORS(app)
app.config['UPLOAD_FOLDER'] = './uploads'
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # 10MB

# API Keys
DEEPSEEK_API_KEY = "sk-5ec306ca459349a4b9caa8e20454b4be"
API_URL = "https://api.deepseek.com/v1/chat/completions"
INTENT_MODEL_URL = 'http://172.17.0.3:10052/v1'
INTENT_MODEL_URL = 'http://qjq-n7527ixuk1ttjbfq5-fzbecewvq-custom.service.onethingrobot.com/v1'

# 添加AiPPT API相关配置
AIPPT_APP_ID = "XXXXXXXX"  # 请替换为实际的APP ID
AIPPT_API_SECRET = "XXXXXXXXXXXXXXXXXXXXXXXX"  # 请替换为实际的API Secret
AIPPT_TEMPLATE_ID = "20240718489569D"  # PPT模板ID

# 初始化智谱AI客户端
#client = ZhipuAI(api_key="4701d076f5fc5b79919d7ed7d1fd220e.cZGekQXg0fypWIkY") 
#a78b2a8931a445e98f12a128fdc651ec.aMDS3pYc1xjpVcMw # 请填写您自己的APIKey
client = ZhipuAI(api_key="a78b2a8931a445e98f12a128fdc651ec.aMDS3pYc1xjpVcMw") 

file_storage = {}
scheduler = BackgroundScheduler()

# 文件清理任务
def clean_files():
    now = datetime.now()
    expired = []
    for fid, data in file_storage.items():
        if (now - datetime.fromisoformat(data['upload_time'])).seconds > 3600:
            expired.append(fid)
    for fid in expired:
        try:
            os.remove(file_storage[fid]['path'])
            del file_storage[fid]
        except: pass

scheduler.add_job(clean_files, 'interval', minutes=60)
scheduler.start()

# 辅助函数
def analyze_data(df, question):
    """生成数据分析提示"""
    columns = df.columns.tolist()
    sample = df.head(3).to_dict()
    return f"""你是一个专业数据分析师，请严格根据以下数据回答问题：
    
数据集特征：{columns}
数据样例：{sample}

用户问题：{question}

回答要求：
1. 基于数据实际情况
2. 包含关键指标计算
3. 使用Markdown格式
4. 重要数据用**加粗**强调"""
# 企业微信机器人 Webhook
webhook_url = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=cae9ff6f-7125-4fe8-bec6-1d2aa0743231"

UPLOAD_FOLDER = './uploads'
ALLOWED_EXTENSIONS = {'xlsx'}

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_model(openai_api_base):
    """
    模型url
    """
    print('openai_api_base:')
    print(openai_api_base)
    httpx_client = httpx.Client(http2=True, verify=False)
    openai_api_key = "EMPTY"
    openai_api_base = openai_api_base

    model = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
        http_client=httpx_client
    )
    return model

def generate_story_with_custom_model(prompt, timeout=500):
    try:
        qw_model = get_model(INTENT_MODEL_URL)
        print("--qwen model--")
        print(qw_model)

        prompts = '''这是一个专业的故事生成助手，请根据用户需求生成高质量的故事。

<文本>
"{text}"

'''
        prompt_template = ChatPromptTemplate.from_template(prompts)
        new_prompt = prompt_template.invoke({'text': prompt}).to_string()[:7000]
        
        start_time = time.time()
        print("new_prompt test:")
        print(new_prompt)
        chat_response = qw_model.chat.completions.create(
            model='/root/LLaMA-Factory/qwen_model',
            messages=[
                {"role": "system", "content": 'you are a professional storyteller'},
                {"role": "user", "content": new_prompt}
            ],
            top_p=0.00000001,
            max_tokens=2000,
            temperature=0.1
        )
        
        #if time.time() - start_time > timeout:
        #    return None
        print("chat_response:")
        print(chat_response)
        print("--content--")
        print(chat_response.choices[0].message.content)
        return chat_response.choices[0].message.content
    except Exception as e:
        print(f"Custom model error: {e}")
        return None

def call_deepseek(prompt, timeout=3):
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1
    }
    try:
        start_time = time.time()
        response = requests.post(API_URL, json=data, headers=headers, timeout=timeout)
        if time.time() - start_time > timeout:
            return None
        print("--deepseek response--")
        print(response)
        res = response.json()['choices'][0]['message']['content']
        return res
    except (Timeout, requests.exceptions.RequestException):
        return None

def evaluate_story(story):
    evaluation_prompt = f"""Please evaluate the following Patagonia story based on these criteria:
1. Authenticity and alignment with Patagonia's values
2. Writing style and quality
3. Story structure and engagement
4. CSR principles integration
5. Overall impact and message

Story to evaluate:
{story}

Please provide a detailed evaluation with scores for each criterion (1-10) and specific feedback for improvement."""

    return call_deepseek(evaluation_prompt)

def send_text_message(content, mentioned_mobile_list=None):
    data = {
        "msgtype": "text",
        "text": {
            "content": content,
            "mentioned_mobile_list": mentioned_mobile_list or []
        }
    }
    response = requests.post(webhook_url, json=data)
    return response.json()

def send_markdown_message(content):
    data = {
        "msgtype": "markdown",
        "markdown": {
            "content": content
        }
    }
    response = requests.post(webhook_url, json=data)
    return response.json()

def excel_to_text(file_path):
    # 读取Excel并转为文本
    df = pd.read_excel(file_path)
    text = df.to_string(index=False)
    return text

@app.route('/')
def index():
    return app.send_static_file('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    user_message = data.get('message', '').strip().lower()
    
    def generate():
        is_patagonia_query = "patagonia" in user_message

        if is_patagonia_query:
            system_prompt = """You are a professional storyteller specializing in Patagonia's brand stories. Your task is to generate high-quality stories that align with Patagonia's Corporate Background and CSR principles.

Requirements:
1. Generate stories that are indistinguishable from authentic Patagonia stories in terms of quality and style
2. Follow the writing patterns and length of existing Patagonia stories
3. Create diverse stories based on detailed content rather than just a few themes
4. All output must be in English
5. Focus on authenticity and alignment with Patagonia's values and mission

Please ensure your response follows these guidelines and maintains Patagonia's authentic voice and storytelling style."""
            
            # 首先尝试使用自研模型生成故事
            story_content = generate_story_with_custom_model(user_message, timeout=1000)
            print("story_content:")
            print(story_content)
            # 如果自研模型失败，使用GLM-4生成故事
            if not story_content:
                response = client.chat.completions.create(
                    model="glm-4",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_message},
                    ],
                    stream=True,
                )
                
                story_content = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        story_content += content
                        yield f"data: {json.dumps({'content': content})}\n\n"
            else:
                # 如果自研模型成功，直接返回生成的内容
                yield f"data: {json.dumps({'content': story_content})}\n\n"

            # 故事生成完毕后统一发送
            if story_content:
                send_text_message(story_content)
        else:
            system_prompt = """你是一个乐于回答各种问题的小助手，你的任务是提供专业、准确、有洞察力的建议。请使用 Markdown 格式来组织你的回答，并严格遵循以下规则：
1. 标题和内容之间不要空行，标题后直接跟内容
2. 使用标题（#）来区分主要部分，标题后立即跟内容，不要空行
3. 使用列表（- 或 1.）来组织要点，列表项之间不空行
4. 使用引用（>）来强调重要内容
5. 使用代码块（```）来展示代码或技术内容
6. 使用粗体（**）来强调关键词
7. 使用斜体（*）来标注补充说明
8. 使用分隔线（---）来区分不同部分
9. 确保标题和内容之间没有空行，直接连接
10. 避免在任何地方使用多余的空行
请确保回答结构清晰，但内容紧凑，避免任何不必要的空行。"""

            response = client.chat.completions.create(
                model="glm-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message},
                ],
                stream=True,
            )
            
            answer_content = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    answer_content += content
                    yield f"data: {json.dumps({'content': content})}\n\n"
            # 普通问答生成完毕后统一发送
            if answer_content:
                send_text_message(answer_content)
        
        # 如果用户问题中包含"patagonia"，进行故事评估
        if is_patagonia_query and story_content:
            evaluation = evaluate_story(story_content)
            if evaluation:  # 只有在成功获取评估结果时才添加评估内容
                evaluation_content = "\n\n---\n\nStory Evaluation:\n" + evaluation
                yield f"data: {json.dumps({'content': evaluation_content})}\n\n"
        
        yield "data: [DONE]\n\n"
    
    return Response(generate(), mimetype='text/event-stream')

@app.route('/upload_excel', methods=['POST'])
def upload_excel():
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)

        # Excel转文本
        excel_text = excel_to_text(file_path)

        # 用大模型分析
        # 你可以直接用现有的分析函数，比如调用GLM-4或自研模型
        # 这里以GLM-4为例
        system_prompt = "请对以下Excel内容进行分析，总结主要信息并给出建议："
        response = client.chat.completions.create(
            model="glm-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": excel_text},
            ],
            stream=False,
        )
        result = response.choices[0].message.content

        # 推送到企业微信
        send_text_message(result)

        return jsonify({'result': result})
    else:
        return jsonify({'error': 'Invalid file type'}), 400

# 路由
@app.route('/upload_excelss', methods=['POST'])
def upload_excelss():
    if 'file' not in request.files:
        return jsonify({'error': '未选择文件'}), 400
    
    file = request.files['file']
    if not file or file.filename == '':
        return jsonify({'error': '无效文件'}), 400
    
    try:
        # 保存文件
        file_id = str(uuid.uuid4())
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
        file.save(filepath)
        
        # 读取元数据
        df = pd.read_excel(filepath)
        if len(df) > 100000 or len(df.columns) > 50:
            os.remove(filepath)
            return jsonify({'error': '数据量过大'}), 400
        
        # 存储元数据
        file_storage[file_id] = {
            'path': filepath,
            'columns': df.columns.tolist(),
            'sample': df.head(3).to_dict(),
            'upload_time': datetime.now().isoformat(),
            'size': f"{os.path.getsize(filepath)/1024:.1f}KB"
        }
        
        return jsonify({
            'fileId': file_id,
            'fileName': filename,
            'fileSize': file_storage[file_id]['size'],
            'columns': file_storage[file_id]['columns']
        })
    except Exception as e:
        return jsonify({'error': f'文件处理失败: {str(e)}'}), 500

def recognize_intent(text):
    """识别用户意图"""
    prompt = f"""请分析以下用户输入，识别其意图：

用户输入：{text}

可能的意图包括：
1. 分析意图：用户想要分析数据，可能包含文件上传
2. ROI预测意图：用户想要进行ROI预测分析，可能包含文件上传
3. 执行意图：用户想要执行特定操作，如生成广告参数、自动投放等
用户可以同时有多个意图，比如同时需要分析和ROI预测。
请以JSON格式返回识别结果，格式如下：
{{
    "intents": ["分析意图", "ROI预测意图", "执行意图"],  // 可以包含多个意图
    "has_file": true/false,
    "details": "具体执行内容"
}}
"""
    try:
        response = client.chat.completions.create(
            model="glm-4",
            messages=[{"role": "user", "content": prompt}],
            stream=False
        )
        result = response.choices[0].message.content
        # 解析JSON结果
        intent_data = json.loads(result)
        return intent_data
    except Exception as e:
        print(f"Error recognizing intent: {e}")
        return {"intent": "[分析意图]", "has_file": False, "details": ""}

def handle_analysis_intent(text, file_data=None):
    """处理分析意图"""
    if file_data:
        # 有文件时的分析逻辑
        df = pd.read_excel(file_data['path'])
        system_prompt = analyze_data(df, text)
    else:
        # 无文件时的分析逻辑
        system_prompt = f"""你是一个专业的数据分析师，请基于以下问题进行分析：

问题：{text}

要求：
1. 提供专业的分析建议
2. 使用Markdown格式
3. 结构清晰，层次分明
4. 包含具体的数据支持
"""
    
    response = client.chat.completions.create(
        model="glm-4",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ],
        stream=False
    )
    return response.choices[0].message.content

def handle_roi_intent(text, file_data=None):
    """处理ROI预测意图"""
    if file_data:
        # 有文件时的ROI预测逻辑
        df = pd.read_excel(file_data['path'])
        system_prompt = f"""你是一个专业的ROI分析师，请基于以下数据预测ROI：

数据特征：{df.columns.tolist()}
数据样例：{df.head(3).to_dict()}

问题：{text}

要求：
1. 提供详细的ROI预测分析
2. 包含关键指标计算
3. 使用Markdown格式
4. 重要数据用**加粗**强调
"""
    else:
        # 无文件时的ROI预测逻辑
        system_prompt = f"""你是一个专业的ROI分析师，请基于以下问题构建假设并进行ROI预测：

问题：{text}

要求：
1. 构建合理的假设场景
2. 提供详细的ROI预测分析
3. 包含关键指标计算
4. 使用Markdown格式
5. 重要数据用**加粗**强调
"""
    
    response = client.chat.completions.create(
        model="glm-4",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ],
        stream=False
    )
    return response.choices[0].message.content

def handle_execution_intent(text):
    """处理执行意图"""
    prompt = f"""请根据以下执行需求生成具体的执行方案：

需求：{text}

要求：
1. 如果是生成Google Ads参数，请提供完整的参数配置
2. 如果是自动投放，请说明投放策略和步骤
3. 如果是其他执行需求，请提供详细的执行方案
4. 使用Markdown格式
5. 结构清晰，层次分明
"""
    
    response = client.chat.completions.create(
        model="glm-4",
        messages=[{"role": "user", "content": prompt}],
        stream=False
    )
    return response.choices[0].message.content

@app.route('/chatss', methods=['POST'])
def chat_handlerss():
    data = request.json
    question = data.get('message', '').strip()
    file_id = data.get('fileId')
    
    def generate():
        try:
            # 识别意图
            intent_data = recognize_intent(question)
            print("--intent_data--")
            print(intent_data)
            
            # 获取文件数据（如果有）
            file_data = None
            if file_id and file_id in file_storage:
                file_data = file_storage[file_id]
            
            # 根据意图处理请求
            if intent_data["intent"] == "分析意图":
                result = handle_analysis_intent(question, file_data)
            elif intent_data["intent"] == "ROI预测意图":
                result = handle_roi_intent(question, file_data)
            elif intent_data["intent"] == "执行意图":
                result = handle_execution_intent(question)
            else:
                result = "无法识别意图，请重新输入"
            
            # 流式返回结果
            for chunk in result.split('\n'):
                # 使用字符串连接而不是f-string来避免反斜杠问题
                yield "data: " + json.dumps({'content': chunk + '\n'}) + "\n\n"
            
            # 生成PPT并发送到企业微信
            if result:
                report_title = generate_report_title(question, result)
                report_title = report_title.replace("\"", "")
                pptx_path = text_to_pptx(result, title="AI分析报告", custom_prefix=report_title)
                media_id = upload_file_to_wecom(pptx_path, webhook_url)
                #send_file_to_wecom(media_id, webhook_url)
                
        except Exception as e:
            yield "data: " + json.dumps({'content': f'⚠️ 处理失败: {str(e)}'}) + "\n\n"
        finally:
            yield "data: [DONE]\n\n"
    
    return Response(generate(), mimetype='text/event-stream')

import tempfile
import os
from pptx import Presentation
from pptx.util import Pt

def generate_report_title(question, answer):
    """生成6字标题"""
    prompt = f"""请根据以下问题和答案生成一个准确、吸引人的标题,标题不超过6个字：

问题：{question}
答案：{answer}

要求：
1. 要概括内容核心
2. 要简洁有力
3. 要正式专业
4. 要表明具体方向

例子：
1、询盘AI策略优化_20250530
"""

    try:
        response = client.chat.completions.create(
            model="glm-4",
            messages=[{"role": "user", "content": prompt}],
            stream=False
        )
        title = response.choices[0].message.content.strip()

        print("--title result--")
        print(question)
        print(answer)
        print(title)
        # 确保返回的是6个汉字
        #if len(title) != 6:
        #    return "数据分析报告"
        return title
    except:
        return "数据分析报告"

def structure_content_for_ppt(text, title):
    """使用GLM-4对内容进行结构化，生成适合PPT展示的格式"""
    prompt = f"""请将以下内容重新组织为适合PPT展示的格式：

原始内容：
{text}

要求：
1. 保持专业性和逻辑性
2. 每个段落应该是一个独立的要点
3. 使用简洁的语言
4. 确保内容层次分明
5. 每个段落都应该有一个清晰的主题,标题不超过6个字
6. 使用"#"标记每个段落的标题
7. 标题要简短有力
8. 内容要点化，便于PPT展示

请按照以下格式输出：
# 标题1
内容1

# 标题2
内容2

...以此类推
"""
    
    try:
        response = client.chat.completions.create(
            model="glm-4",
            messages=[{"role": "user", "content": prompt}],
            stream=False
        )
        structured_content = response.choices[0].message.content

        print("--structured_content--")
        print(structured_content)
        return structured_content
    except Exception as e:
        print(f"Error structuring content: {e}")
        return text

def text_to_pptx(text, title="AI分析报告", custom_prefix=None):
    """使用AiPPT API创建PPT"""
    try:
        # 初始化AiPPT客户端
        ai_ppt = AIPPT(AIPPT_APP_ID, AIPPT_API_SECRET, text, AIPPT_TEMPLATE_ID)
        
        # 创建PPT生成任务
        task_id = ai_ppt.create_task()
        if not task_id:
            raise Exception("创建PPT任务失败")
        
        # 获取PPT下载链接
        ppt_url = ai_ppt.get_result(task_id)
        if not ppt_url:
            raise Exception("获取PPT失败")
        
        # 下载PPT文件
        response = requests.get(ppt_url)
        if response.status_code != 200:
            raise Exception("下载PPT失败")
        
        # 保存到临时文件
        if custom_prefix:
            fd, tmp_path = tempfile.mkstemp(suffix='.pptx', prefix=custom_prefix + '_')
        else:
            fd, tmp_path = tempfile.mkstemp(suffix='.pptx')
        
        with open(tmp_path, 'wb') as f:
            f.write(response.content)
        
        os.close(fd)
        return tmp_path
        
    except Exception as e:
        print(f"Error generating PPT: {e}")
        # 如果API调用失败，回退到原来的PPT生成方法
        return text_to_pptxs(text, title)

def text_to_pptxs(text, title="AI分析报告"):
    prs = Presentation()
    # 标题页
    slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(slide_layout)
    slide.shapes.title.text = title
    slide.placeholders[1].text = "由AI自动生成"

    # 内容页
    slide_layout = prs.slide_layouts[1]
    for idx, para in enumerate(text.split('\n\n')):
        slide = prs.slides.add_slide(slide_layout)
        slide.shapes.title.text = f"第{idx+1}部分"
        content = slide.placeholders[1]
        content.text = para.strip()
        for paragraph in content.text_frame.paragraphs:
            for run in paragraph.runs:
                run.font.size = Pt(18)

    # 保存到临时文件
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.pptx')
    prs.save(tmp.name)
    tmp.close()
    return tmp.name


def upload_file_to_wecom(filepath, webhook_url):
    # 获取上传url
    upload_url = webhook_url.replace('/send?', '/upload_media?') + '&type=file'
    
    with open(filepath, 'rb') as f:
        print("--filepath--")
        print(filepath)
        print(f)
        files = {'media': f}
        resp = requests.post(upload_url, files=files)
    media_id = resp.json().get('media_id')
    return media_id

def send_file_to_wecom(media_id, webhook_url):
    data = {
        "msgtype": "file",
        "file": {
            "media_id": media_id
        }
    }
    resp = requests.post(webhook_url, json=data)
    return resp.json()

@app.route('/wecom_webhook', methods=['POST'])
def wecom_webhook():
    """处理企业微信机器人的webhook请求"""
    try:
        # 获取请求数据
        data = request.json
        print("--wecom webhook data--")
        print(data)
        
        # 提取消息内容
        msg_type = data.get('msgtype')
        if msg_type == 'text':
            content = data.get('text', {}).get('content', '').strip()
            # 移除@机器人的标记
            content = content.replace('@MarketManus', '').strip()
            
            # 检查是否有文件上传
            file_id = None
            if 'file' in data:
                file_id = data['file'].get('file_id')
            
            # 调用现有的聊天处理逻辑
            chat_data = {
                'message': content,
                'fileId': file_id
            }
            
            # 使用现有的chat_handlerss函数处理请求
            response = chat_handlerss()
            return response
            
        else:
            return jsonify({'error': '不支持的消息类型'}), 400
            
    except Exception as e:
        print(f"Error processing WeChat Work webhook: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8080) 